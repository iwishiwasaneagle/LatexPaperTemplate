\section{Fully Connected Network}

The \nomdef{Fully Connected Network}{FCN} (also known as the Multi-Layer Perceptron, Feedforward Neural Network, or Artificial Neural Network) is the basis of most machine learning models. At its core, the FCN is made up of many individual neurons.

The basis of the neuron is the perceptron which attempt to model a human brain. The perceptron can be defined by
\begin{gather}
    \label{eqn:perceptron}
    \begin{split}
        \vec y = H(\mat W \cdot \vec x + b) \\
        H(x) =
        \begin{cases}
            1,& x \geq 0 \\
            0,& x<0
        \end{cases}
    \end{split}
\end{gather}
where $H$ is the Heaviside step function, $W$ are the weights, and $b$ is the bias. $\vec x$ and $\vec W^T$ are of the shape $(1, m)$ and $m$ depends on the architecture.

The derivative of the Heaviside step function is the dirac delta function,
\begin{equation}
    \frac{\partial}{\partial x} H(x) = \delta (x)
\end{equation}
making this formulation unsuitable for backpropagation via gradient descent.

\begin{gather}
    \label{eqn:neuron}
    \vec y = f(\mat W \cdot \vec x + b)
\end{gather}
where $f$ is an activation function, and $W$, $b$ are the same as in \autoref{eqn:perceptron}.

\note{Paragraph on popular activation functions. Sigmoid, ReLU, tanh, etc.}
\note{Section on backpropagation in general. Maybe include LSTM, RNN, and GRU backpropagation? Or just explain how it works with non-neuron units.
    % https://theorydish.blog/2021/12/16/backpropagation-\%E2\%89\%A0-chain-rule/
    % https://math.libretexts.org/Bookshelves/Calculus/Calculus_(OpenStax)/14\%3A_Differentiation_of_Functions_of_Several_Variables/14.05\%3A_The_Chain_Rule_for_Multivariable_Functions
}

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/fcn_more_detail.tikz.tex}
    \caption{<caption>}
    \label{<label>}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/perceptron.tikz.tex}
    \caption{<caption>}
    \label{<label>}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/fcn.tikz.tex}
    \caption{<caption>}
    \label{<label>}
\end{figure}

\section{Convolution Neural Network}

\begin{equation}
    y(i, j) = \sum^{M-1}_{m=0}\sum^{N-1}_{n=0} K(m,n)x(i-m, j-n)~\forall~i \in \mathbb{R}^{M-K+1}, j \in \mathbb{R}^{N-L+1}
\end{equation}
where
\begin{itemize}
    \item $x$ is the input 2D image of shape $K \times L$,
    \item $K$ is the convolution kernel of shape $M \times N$,
    \item $y$ is the output image of shape $(M-K+1) \times (N-L+1)$.
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/cnn.tikz.tex}
    \caption{<caption>}
    \label{<label>}
\end{figure}

\section{Recurrent Neural Network}

The \nomdef{Recurrent Neural Network}{RNN} implementation used in this work is the Elman network.

For each element in the input sequence, each layer computes the function
\begin{align}
    h_t &= \tanh (\gamma_h) \\
    \gamma_h &= x_t W^T_{xh} + h_{t-1}W_{hh}^T + b_h
\end{align}
where
\begin{itemize}
    \item $h_t$ is the hidden state at time $t$,
    \item $x_t$ is the input at time $t$,
    \item $h_{t-1}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $t=0$,
    \item $W$ and $b$ are parameter matrices and vectors (weights and biases).
\end{itemize}
The \nomdef{Recurrent Neural Network}{RNN} unit internal structure can be seen in \autoref{fig:rnn_unit}.

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/rnn.tikz.tex}
    \caption{The Recurrent Neural Network unit internal structure from \autoref{eqn:rnn}}
    \label{fig:rnn_unit}
\end{figure}

\subsection{Backpropagation}

Gradient with respect to the weights $W_{xh}$, and $W_{hh}$ and bias $b_h$
\begin{align}
    \pdv{E}{W_{xh}} &= \pdv{E}{h_t}\pdv{h_t}{W_{xh}} = \pdv{E}{h_t} x_t (1-h_t^2) \\
    \pdv{E}{W_{hh}} &= \pdv{E}{h_t}\pdv{h_t}{W_{hh}} = \pdv{E}{h_t} h_{t-1} (1-h_t^2) \\
    \pdv{E}{b_h} &= \pdv{E}{h_t}\pdv{h_t}{b_h} = \pdv{E}{h_t}~(1-h_t^2) \\
\end{align}

Gradient with respect to the previous hidden state $h_{t-1}$
\begin{equation}
    \pdv{E}{h_{t-1}} = \pdv{E}{h_t} \pdv{h_t}{h_{t-1}} = \pdv{E}{h_t}~(1-h_t^2) W_{hh}^T
\end{equation}

\section{Long Short-Term Memory}

For each element in the input sequence, each layer computes the function
\begin{align}
    \label{eqn:lstm}
    \begin{split}
        i_t &= \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_{i}) \\
        f_t &= \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_{f}) \\
        g_t &= \tanh(W_{xg}x_t +  W_{hg}h_{t-1} + b_{g}) \\
        o_t &= \sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_{o}) \\
        c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
        h_t &= o_t \odot \tanh(c_t)
    \end{split}
\end{align}
where
\begin{itemize}
    \item $h_t$ is the hidden state at time $t$,
    \item $c_t$ is the cell state at time $t$,
    \item $x_t$ is the input at time $t$,
    \item $h_{t-1}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $t=0$,
    \item $i_t$, $f_t$, $g_t$, and $o_t$ are the input, forget, cell, and output gates respectively,
    \item $\sigma$ is the sigmoid function,
    \item $\odot$ is the hadamard product,
    \item $W$ and $b$ are parameter matrices and vectors (weights and biases).
\end{itemize}
The \nomdef{Long Short-Term Memory}{LSTM} unit internal structure can be seen in \autoref{fig:lstm_unit}.

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/lstm.tikz.tex}
    \caption{The Long Short-Term Memory unit internal structure from \autoref{eqn:lstm}}
    \label{fig:lstm_unit}
\end{figure}

\subsection{Backpropagation}

Gradient with respect to the output gate $o_t$
\newcommand{\pdvEot}{\pdv{E}{h_t}\tanh(c_t)}
\begin{equation}
    \pdv{E}{o_t}
    =
    \pdv{E}{h_t}\pdv{h_t}{o_t}
    =
    \pdvEot
\end{equation}

Gradient with respect the cell state vector $c_t$
\newcommand{\pdvEct}{\pdv{E}{h_t} o_t \sech^2 (c_t)}
\begin{equation}
    \pdv{E}{c_t}
    =
    \pdv{E}{h_t}\pdv{h_t}{c_t}
    =
    \pdvEct
\end{equation}

Gradient with respect to the cell input gate $g_t$
\newcommand{\pdvEgt}{\pdvEct i_t}
\begin{equation}
    \pdv{E}{g_t} = \pdv{E}{c_t}\pdv{c_t}{g_t} = \pdvEgt
\end{equation}

Gradient with respect to the input gate $i_t$
\newcommand{\pdvEit}{\pdvEct g_t}
\begin{equation}
    \pdv{E}{i_t} = \pdv{E}{c_t}\pdv{c_t}{i_t} = \pdvEit
\end{equation}

Gradient with respect to the forget gate $f_t$
\newcommand{\pdvEft}{\pdvEct c_{t-1}}
\begin{equation}
    \pdv{E}{f_t} = \pdv{E}{c_t}\pdv{c_t}{f_t} = \pdvEft
\end{equation}

Gradient with respect to the weights $W_{hi}$, $W_{xi}$, $W_{hf}$, $W_{xf}$, $W_{hg}$, $W_{xh}$, $W_{ho}$, and $W_{xo}$ and biases $b_{i}$,  $b_{f}$, $b_{g}$, $b_{o}$
\newcommand{\pdvsigmoid}[1]{\sigma(#1)(1-\sigma(#1))}
\newcommand{\pdvtanh}[1]{\sech^2(#1)}
\renewcommand{\pdvEit}{\pdv{E}{i_t}}
\renewcommand{\pdvEft}{\pdv{E}{f_t}}
\renewcommand{\pdvEgt}{\pdv{E}{g_t}}
\renewcommand{\pdvEot}{\pdv{E}{o_t}}
\begin{align}
    \begin{split}
        \pdv{E}{W_{hi}} &= \pdv{E}{i_t}\pdv{x_t}{W_{hi}} =  \pdvEit h_{t-1} \pdvsigmoid{\gamma_i}\\
        \pdv{E}{W_{xi}} &= \pdv{E}{i_t}\pdv{x_t}{W_{xi}} =  \pdvEit x_t \pdvsigmoid{\gamma_i}\\
        \pdv{E}{b_{i}} &= \pdv{E}{i_t}\pdv{x_t}{b_{i}} =  \pdvEit \pdvsigmoid{\gamma_i}\\
        %
        \pdv{E}{W_{hf}} &= \pdv{E}{f_t}\pdv{f_t}{W_{hf}} = \pdvEft h_{t-1} \pdvsigmoid{\gamma_f}\\
        \pdv{E}{W_{xf}} &= \pdv{E}{f_t}\pdv{f_t}{W_{xf}} = \pdvEft x_t \pdvsigmoid{\gamma_f}\\
        \pdv{E}{b_{f}} &= \pdv{E}{f_t}\pdv{f_t}{b_{f}} = \pdvEft \pdvsigmoid{\gamma_f}\\
        %
        \pdv{E}{W_{hg}} &= \pdv{E}{g_t}\pdv{g_t}{W_{hg}} = \pdvEgt h_{t-1} (1-g_t^2)  \\
        \pdv{E}{W_{xg}} &= \pdv{E}{g_t}\pdv{g_t}{W_{xg}} = \pdvEgt x_t(1-g_t^2)\\
        \pdv{E}{b_{g}} &= \pdv{E}{g_t}\pdv{g_t}{b_{g}} = \pdvEgt ~ (1-g_t^2)\\
        %
        \pdv{E}{W_{ho}} &= \pdv{E}{o_t}\pdv{o_t}{W_{ho}} = \pdvEot h_{t-1} \pdvsigmoid{\gamma_f}\\
        \pdv{E}{W_{xo}} &= \pdv{E}{o_t}\pdv{o_t}{W_{xo}} = \pdvEot x_t \pdvsigmoid{\gamma_f}\\
        \pdv{E}{b_{o}} &= \pdv{E}{o_t}\pdv{o_t}{b_{o}} = \pdvEot \pdvsigmoid{\gamma_f}\\
    \end{split}
\end{align}
where $\gamma_a = W_{xa}x_t + b_{a} + W_{ha}h_{t-1}$

Gradient with respect to the previous hidden state $h_{t-1}$
\begin{equation}
    \pdv{E}{h_{t-1}} = \pdv{E}{o_t} \pdv{o_t}{h_{t-1}} = \pdvEot W_{hg} (1-g_t^2)
\end{equation}

\section{Gated Recurrent Unit}
For each element in the input sequence, each layer computes the following function:
\begin{align}
    \label{eqn:gru}
    \begin{split}
        r_t &= \sigma(W_{xr}x_t +  W_{hr}h_{t-1} + b_{r}) \\
        z_t &= \sigma(W_{xz}x_t + W_{hz}h_{t-1} + b_{z}) \\
        n_t &= \tanh(W_{xn}x_t + b_{xn} + r_t \odot (W_{hn}h_{t-1} + b_{hn})) \\
        h_t &= (1 - z_t) \odot n_t + z_t \odot h_{t-1}
    \end{split}
\end{align}
where
\begin{itemize}
    \item $h_t$ is the hidden state at time $t$,
    \item $c_t$ is the cell state at time $t$,
    \item $x_t$ is the input at time $t$,
    \item $h_{t-1}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $t=0$,
    \item $r_t$, $z_t$, and $n_t$ are the reset, update, and new gates respectively,
    \item $\sigma$ is the sigmoid function,
    \item $\odot$ is the hadamard product,
    \item $W$ and $b$ are parameter matrices and vectors (weights and biases).
\end{itemize}
The \nomdef{Gated Recurrent Unit}{GRU} internal structure can be seen in \autoref{fig:gru_unit}.

\begin{figure}[htbp]
    \centering
    \includetikz{diagrams/gru.tikz.tex}
    \caption{The Gated Recurrent Unit internal structure from \autoref{eqn:gru}}
    \label{fig:gru_unit}
\end{figure}

\subsection{Backpropagation}
Gradient with respect to the new gate $n_t$
\newcommand{\pdvEnt}{\pdv{E}{h_t}~(1-z_t)}
\begin{equation}
    \pdv{E}{n_t} = \pdv{E}{h_t} \pdv{h_t}{n_t} = \pdvEnt
\end{equation}

Gradient with respect to the update gate $z_t$
\newcommand{\pdvEzt}{\pdv{E}{h_t}~(h_{t-1} - n_t)}
\begin{equation}
    \pdv{E}{z_t} = \pdv{E}{h_t} \pdv{h_t}{z_t} = \pdvEzt
\end{equation}

Gradient with respect to the reset gate $r_t$
\newcommand{\pdvErt}{\pdvEnt~(W_{hn}h_{t-1}+b_{hn})(1-n_t^2)}
\begin{equation}
    \pdv{E}{r_t} = \pdv{E}{n_t} \pdv{n_t}{r_t} = \pdvErt
\end{equation}

Gradient with respect to the weights $W_{hr}$, $W_{xr}$, $W_{hz}$, $W_{zx}$, $W_{hn}$, $W_{xn}$ and biases $b_{r}$,  $b_{z}$, $b_{xn}$, and $b_{hn}$
\renewcommand{\pdvErt}{\pdv{E}{r_t}}
\renewcommand{\pdvEzt}{\pdv{E}{z_t}}
\renewcommand{\pdvEnt}{\pdv{E}{n_t}}
\begin{align}
    \begin{split}
        \pdv{E}{W_{hr}} &= \pdv{E}{r_t}\pdv{r_t}{W_{hr}} =  \pdvErt h_{t-1} \pdvsigmoid{\gamma_r}\\
        \pdv{E}{W_{xr}} &= \pdv{E}{r_t}\pdv{r_t}{W_{xr}} =  \pdvErt x_t \pdvsigmoid{\gamma_r}\\
        \pdv{E}{b_{r}} &= \pdv{E}{r_t}\pdv{r_t}{b_{r}} =  \pdvErt \pdvsigmoid{\gamma_r}\\
        %
        \pdv{E}{W_{hz}} &= \pdv{E}{z_t}\pdv{z_t}{W_{hz}} = \pdvEzt h_{t-1} \pdvsigmoid{\gamma_z}\\
        \pdv{E}{W_{xz}} &= \pdv{E}{z_t}\pdv{z_t}{W_{xz}} = \pdvEzt x_t \pdvsigmoid{\gamma_z}\\
        \pdv{E}{b_{z}} &= \pdv{E}{z_t}\pdv{z_t}{b_{z}} = \pdvEzt \pdvsigmoid{\gamma_z}\\
        %
        \pdv{E}{W_{hn}} &= \pdv{E}{n_t}\pdv{n_t}{W_{hn}} = \pdvEnt~(r_t \odot h_{t-1}) (1-n_t^2)  \\
        \pdv{E}{W_{xn}} &= \pdv{E}{n_t}\pdv{n_t}{W_{xn}} = \pdvEnt x_t (1-n_t^2)\\
        \pdv{E}{b_{hn}} &= \pdv{E}{n_t}\pdv{n_t}{b_{hn}} = \pdvEnt~r_t (1-n_t^2)\\
        \pdv{E}{b_{xn}} &= \pdv{E}{n_t}\pdv{n_t}{b_{xn}} = \pdvEnt~(1-n_t^2)\\
    \end{split}
\end{align}
where $\gamma_a = W_{xa}x_t + b_{a} + W_{ha}h_{t-1}$

\note{$\pdv{E}{h_{t-1}}$?}